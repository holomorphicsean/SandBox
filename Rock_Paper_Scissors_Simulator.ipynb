{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rock Paper Scissors Simulator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPzSq31N1VNpqEUFPrlco7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/holomorphicsean/SandBox/blob/main/Rock_Paper_Scissors_Simulator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP_pF2CXW7pY"
      },
      "source": [
        "# VARIABLE AND FUNCTION DEFINITIONS\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muba5gabXAsX"
      },
      "source": [
        "import random\n",
        "\n",
        "# RPS payoff table\n",
        "# example: payoff_table[ROCK][SCISSORS] = [1,-1], implying that, from\n",
        "# player 1's perspective, Rock wins vs Scissors\n",
        "\n",
        "ROCK = 0\n",
        "PAPER = 1\n",
        "SCISSORS = 2\n",
        "\n",
        "payoff = [\n",
        "          [[0,0],[-1,1],[1,-1]],\n",
        "          [[1,-1],[0,0],[-1,1]],\n",
        "          [[-1,1],[1,-1],[0,0]]\n",
        "          ]\n",
        "\n",
        "def regret_table(act1, act2):\n",
        "  # This function takes in an action from a player\n",
        "  # and an opponent (e.g. act1 = 0 and act2 = 2 means ROCK vs SCISSORS)\n",
        "  # and returns a length 3 array of regrets from player 1's perspective\n",
        "\n",
        "  regret = [0, 0, 0]\n",
        "\n",
        "  for i in range(3):\n",
        "    #definition of regret: utility of hypothetical actions vs actual action\n",
        "    regret[i] = payoff[i][act2][0] - payoff[act1][act2][0]\n",
        "  return regret\n",
        "\n",
        "\n",
        "def rm_strategy(regret_sum):\n",
        "  # First step will be to calculate a strategy will be calculated from the \n",
        "  # Regret Sum. Afterwards, that strategy will be used to update the \n",
        "  # average strategy, which is what this function will return.\n",
        "\n",
        "  normalizing_sum = 0\n",
        "  \n",
        "  # Remove negative numbers from regret_sum and place into current strategy\n",
        "  strategy = [i if i > 0 else 0 for i in regret_sum]\n",
        "  normalizing_sum = sum(strategy)\n",
        "\n",
        "  # If our normalizing sum is non-positive, then we return an equal strategy\n",
        "  if normalizing_sum <= 0:\n",
        "    for i in range(3):\n",
        "      strategy_sum[i] += 1/3\n",
        "    return [1/3, 1/3, 1/3]\n",
        "\n",
        "  # Otherwise, update the strategy and the global strategy_sum variable\n",
        "  strategy = [i/normalizing_sum for i in strategy]\n",
        "  for i in range(3):\n",
        "    strategy_sum[i] += strategy[i]\n",
        "\n",
        "  return strategy\n",
        "\n",
        "\n",
        "def action(strategy):\n",
        "  # Using a 3 element normalized array, we select an action based\n",
        "  # on the strategy using cumulative probability\n",
        "\n",
        "  r = random.random() \n",
        "  cumulative_prob = 0 \n",
        "  act = 0 # action index, updates depending on where r falls in the cumul. prob\n",
        "\n",
        "  while act < 2:\n",
        "    cumulative_prob += strategy[act]\n",
        "    if r < cumulative_prob:\n",
        "      break\n",
        "    act += 1\n",
        "\n",
        "  return act\n",
        "\n",
        "def rm_average_strategy(strategy_sum):\n",
        "  # This function takes the strategy sum and normalizes it\n",
        "\n",
        "  average_strategy = [0, 0, 0]\n",
        "  normalizing_sum = 0\n",
        "\n",
        "  normalizing_sum = sum(strategy_sum)\n",
        "\n",
        "  # If normalizing_sum is non-positive then we return an even strategy\n",
        "  if normalizing_sum <= 0:\n",
        "    return [1/3, 1/3, 1/3]\n",
        "\n",
        "  average_strategy = [i/normalizing_sum for i in strategy_sum]\n",
        "\n",
        "  return average_strategy\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqCtsQng026j"
      },
      "source": [
        "# TRAINING ALGORITHM (PROCEDURAL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4a42cr-06Gc",
        "outputId": "7a744120-b20e-4705-ff53-d00c3e8376dd"
      },
      "source": [
        "# Definitions\n",
        "regret_sum = [0, 0, 0]      # current tally of regrets\n",
        "strategy = [0, 0, 0]  # player 1's strategy\n",
        "strategy_sum = [0, 0, 0]    # player 1's strategy sum to get avg later\n",
        "opp_strategy = [.4, .3, .3] # opponent's dummy strategy\n",
        "\n",
        "# Input n number of iterations for program\n",
        "def train(n):\n",
        "\n",
        "  win = 0\n",
        "  draw = 0\n",
        "  loss = 0\n",
        "  for i in range(n):\n",
        "    # Get actions for myself and for my opponent\n",
        "    strategy1 = rm_strategy(regret_sum)\n",
        "    act1 = action(strategy1)\n",
        "    act2 = action(opp_strategy)\n",
        "\n",
        "\n",
        "    # Get regret table for actions\n",
        "    regret = regret_table(act1, act2)\n",
        "\n",
        "    #update win, loss, and draw list\n",
        "    utility = payoff[act1][act2][0]\n",
        "    if utility == 1:\n",
        "      win += 1\n",
        "    if utility == 0:\n",
        "      draw += 1\n",
        "    if utility == -1:\n",
        "      loss += 1\n",
        "\n",
        "    # Update regret sum\n",
        "    for i in range(3):\n",
        "      regret_sum[i] += regret[i]\n",
        "\n",
        "  return [win, draw, loss]\n",
        "\n",
        "wins = train(700000)\n",
        "print(wins, [100*i/sum(wins) for i in wins])\n",
        "rm_average_strategy(strategy_sum)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[279529, 209931, 210540] [39.93271428571428, 29.990142857142857, 30.077142857142857]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9.098783755926613e-05, 0.9998294637188209, 7.95484436198722e-05]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry7K7ejoAiPX"
      },
      "source": [
        "# TRAINING ALGORITHM (OOP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1he8aWcFAn0j",
        "outputId": "77cc1a83-c7cb-4ec6-e0ff-1901c1dc829d"
      },
      "source": [
        "class Player:\n",
        "\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "\n",
        "  def set_strategy(self, strategy):\n",
        "    self.strategy = strategy\n",
        "\n",
        "  def get_strategy(self):\n",
        "    return self.strategy\n",
        "\n",
        "  def roshambo(self):\n",
        "    a = action(strategy)\n",
        "    return a\n",
        "\n",
        "  \n",
        "p1 = Player(\"Me\")\n",
        "p1.set_strategy([1/3, 1/3, 1/3])\n",
        "p1.roshambo()\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    }
  ]
}