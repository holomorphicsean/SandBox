{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rock Paper Scissors Simulator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+d3yB4hFis5kUrGTXX1jW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/holomorphicsean/SandBox/blob/main/Rock_Paper_Scissors_Simulator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP_pF2CXW7pY"
      },
      "source": [
        "# VARIABLE AND FUNCTION DEFINITIONS\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muba5gabXAsX"
      },
      "source": [
        "import random\n",
        "\n",
        "# RPS payoff table\n",
        "# example: payoff_table[ROCK][SCISSORS] = [1,-1], implying that, from\n",
        "# player 1's perspective, Rock wins vs Scissors\n",
        "\n",
        "ROCK = 0\n",
        "PAPER = 1\n",
        "SCISSORS = 2\n",
        "\n",
        "payoff = [\n",
        "          [[0,0],[-1,1],[1,-1]],\n",
        "          [[1,-1],[0,0],[-1,1]],\n",
        "          [[-1,1],[1,-1],[0,0]]\n",
        "          ]\n",
        "\n",
        "def regret_table(act1, act2):\n",
        "  # This function takes in an action from a player\n",
        "  # and an opponent (e.g. act1 = 0 and act2 = 2 means ROCK vs SCISSORS)\n",
        "  # and returns a length 3 array of regrets from player 1's perspective\n",
        "\n",
        "  regret = [0, 0, 0]\n",
        "\n",
        "  for i in range(3):\n",
        "    #definition of regret: utility of hypothetical actions vs actual action\n",
        "    regret[i] = payoff[i][act2][0] - payoff[act1][act2][0]\n",
        "  return regret\n",
        "\n",
        "\n",
        "def rm_strategy(regret_sum):\n",
        "  # First step will be to calculate a strategy will be calculated from the \n",
        "  # Regret Sum. Afterwards, that strategy will be used to update the \n",
        "  # average strategy, which is what this function will return.\n",
        "\n",
        "  normalizing_sum = 0\n",
        "  \n",
        "  # Remove negative numbers from regret_sum and place into current strategy\n",
        "  strategy = [i if i > 0 else 0 for i in regret_sum]\n",
        "  normalizing_sum = sum(strategy)\n",
        "\n",
        "  # If our normalizing sum is non-positive, then we return an equal strategy\n",
        "  if normalizing_sum <= 0:\n",
        "    for i in range(3):\n",
        "      strategy_sum[i] += 1/3\n",
        "    return [1/3, 1/3, 1/3]\n",
        "\n",
        "  # Otherwise, update the strategy and the global strategy_sum variable\n",
        "  strategy = [i/normalizing_sum for i in strategy]\n",
        "  for i in range(3):\n",
        "    strategy_sum[i] += strategy[i]\n",
        "\n",
        "  return strategy\n",
        "\n",
        "\n",
        "def action(strategy):\n",
        "  # Using a 3 element normalized array, we select an action based\n",
        "  # on the strategy using cumulative probability\n",
        "\n",
        "  r = random.random() \n",
        "  cumulative_prob = 0 \n",
        "  act = 0 # action index, updates depending on where r falls in the cumul. prob\n",
        "\n",
        "  while act < 2:\n",
        "    cumulative_prob += strategy[act]\n",
        "    if r < cumulative_prob:\n",
        "      break\n",
        "    act += 1\n",
        "\n",
        "  return act\n",
        "\n",
        "def rm_average_strategy(strategy_sum):\n",
        "  # This function takes the strategy sum and normalizes it\n",
        "\n",
        "  average_strategy = [0, 0, 0]\n",
        "  normalizing_sum = 0\n",
        "\n",
        "  normalizing_sum = sum(strategy_sum)\n",
        "\n",
        "  # If normalizing_sum is non-positive then we return an even strategy\n",
        "  if normalizing_sum <= 0:\n",
        "    return [1/3, 1/3, 1/3]\n",
        "\n",
        "  average_strategy = [i/normalizing_sum for i in strategy_sum]\n",
        "\n",
        "  return average_strategy\n",
        "\n",
        "  # player class implementation\n",
        "  class Player:\n",
        "\n",
        "    def __init__(self, name):\n",
        "      self.name = name\n",
        "\n",
        "    def set_strategy(self, strategy):\n",
        "      self.strategy = strategy\n",
        "\n",
        "    def get_strategy(self):\n",
        "      return self.strategy\n",
        "\n",
        "    def roshambo(self):\n",
        "      a = action(self.strategy)\n",
        "      return a\n"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqCtsQng026j"
      },
      "source": [
        "# Training an AI to use Regret Minimization vs. an AI that has a fixed mixed strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh8sff-9opOe",
        "outputId": "6e71fcb3-8439-4195-bbed-9aad0069b310"
      },
      "source": [
        "# Definitions and player initializations\n",
        "regret_sum = [0, 0, 0]      # global current tally of regrets\n",
        "strategy_sum = [0, 0, 0]    # global player 1's strategy sum to get avg later\n",
        "opp_strategy = [.4, .3, .3]    # opponent's dummy strategy lol\n",
        "\n",
        "#Our two virtual players\n",
        "p1 = Player(\"CFR_AI\")\n",
        "p2 = Player(\"Fixed AI\")\n",
        "\n",
        "# Input n number of iterations for program\n",
        "def train(n):\n",
        "\n",
        "  win = 0\n",
        "  draw = 0\n",
        "  loss = 0\n",
        "  for i in range(n):\n",
        "    # Get actions for myself and for my opponent\n",
        "    strategy1 = rm_strategy(regret_sum)\n",
        "\n",
        "    p1.set_strategy(strategy1)\n",
        "    act1 = p1.roshambo()\n",
        "    p2.set_strategy(opp_strategy)\n",
        "    act2 = p2.roshambo()\n",
        "\n",
        "    # Get regret table for actions\n",
        "    regret = regret_table(act1, act2)\n",
        "\n",
        "    #update win, loss, and draw list\n",
        "    utility = payoff[act1][act2][0]\n",
        "    if utility == 1:\n",
        "      win += 1\n",
        "    if utility == 0:\n",
        "      draw += 1\n",
        "    if utility == -1:\n",
        "      loss += 1\n",
        "\n",
        "    # Update regret sum\n",
        "    for i in range(3):\n",
        "      regret_sum[i] += regret[i]\n",
        "\n",
        "  return [win, draw, loss]\n",
        "\n",
        "wins = train(700000)\n",
        "print(wins, [100*i/sum(wins) for i in wins])\n",
        "rm_average_strategy(strategy_sum)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[280353, 210056, 209591] [40.05042857142857, 30.008, 29.94157142857143]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1904761904761904e-06, 0.999873066847178, 0.00012574267663144272]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amn_ukoGKBn0"
      },
      "source": [
        "# Play Against an AI that makes random moves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFXvWdflKGeB"
      },
      "source": [
        "# Variable Definitions\n",
        "opp_strategy = [.3, .3, .3] # opponent's dummy strategy\n",
        "moves = [\"ROCK\", \"PAPER\", \"SCISSORS\"]\n",
        "\n",
        "num_turns = int(input(\"How many rounds do you want to play? \"))\n",
        "\n",
        "[win, draw, loss] = [0, 0, 0]\n",
        "\n",
        "# Our nemesis\n",
        "p = Player(\"Random_AI\")\n",
        "p.set_strategy([1/3, 1/3, 1/3])\n",
        "\n",
        "i = 0 #iterator\n",
        "while i < num_turns:\n",
        "\n",
        "  myact = input(\"Choose an action: ROCK, PAPER, SCISSORS: \\n\")\n",
        "  if myact == \"ROCK\":\n",
        "    myact = 0\n",
        "  elif myact == \"PAPER\":\n",
        "    myact = 1\n",
        "  elif myact == \"SCISSORS\":\n",
        "    myact = 2\n",
        "  else:\n",
        "    print(\"Not a valid response!\")\n",
        "    continue\n",
        "  \n",
        "  act = p.roshambo()\n",
        "  print(\"Your nemesis chose\", moves[act])\n",
        "\n",
        "  print(moves[myact],\"vs\",moves[act]) # showdown message!\n",
        "\n",
        "  # Our result from the payoff matrix\n",
        "  ut = payoff[myact][act][0]\n",
        "  if ut == 1:\n",
        "    print(\"You win\")\n",
        "    win += 1\n",
        "  elif ut == 0:\n",
        "    print(\"Draw\")\n",
        "    draw += 1\n",
        "  else:\n",
        "    print(\"You lose\")\n",
        "    loss += 1\n",
        "\n",
        "  i += 1\n",
        "\n",
        "# Post-game results\n",
        "print(\"Results:\")\n",
        "if win > loss:\n",
        "  print(\"You won this round.\")\n",
        "elif win == loss:\n",
        "  print(\"You drew this round\")\n",
        "else:\n",
        "  print(\"You lost this round.\")\n",
        "\n",
        "print([win, draw, loss])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqhTpV6qvQjg"
      },
      "source": [
        "# Play against an AI that uses Regret Minimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQQk98yUv5qz"
      },
      "source": [
        "# ---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3E6nX_4vW6k"
      },
      "source": [
        "# Training two AIs that use Regret Minimization against each other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2ry-R9cv9tl"
      },
      "source": [
        "# ---"
      ],
      "execution_count": 133,
      "outputs": []
    }
  ]
}